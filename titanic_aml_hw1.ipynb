{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 3136,
          "databundleVersionId": 26502,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30761,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaldayarova/titanic-survival/blob/main/titanic_aml_hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'titanic:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F3136%2F26502%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240918%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240918T153015Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D764f21ba7a802d15feeff282083811901cc5fc62d1c3ca1e665927e3c3128eb7dcc4fe73ad710a3fbd3ca56fe1d4d7c012d2b1f75a1e887a6129dd7f8b3cbdb44b42917a3cc5b41eb49d18311f4178c90d5161934e691e1ff1d371c409ed08871626ab2f3b159a20e1ebbcf0da6366d2bb3145d99b9f6a50a5b0498b7674b94fba509a46526f897e699dadda225af2835a97eed3d68bed375822e2ff431dde2407956ae4e90b34c5146a7674e0744d021529338733599e1e3553122365f422fa5ca467eafa8a79c4dd055cf45423d6419e29da32e3040afcd6188c6bc128abc5159c29dc9896a04270c96c706eee3192f7366e32aedb56205aacc36bbc8e7070'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "2B_9wCaewXLz"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-09-18T13:45:46.267961Z",
          "iopub.execute_input": "2024-09-18T13:45:46.268921Z",
          "iopub.status.idle": "2024-09-18T13:45:46.723438Z",
          "shell.execute_reply.started": "2024-09-18T13:45:46.268861Z",
          "shell.execute_reply": "2024-09-18T13:45:46.722126Z"
        },
        "trusted": true,
        "id": "no6VDnhgwXL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating variables for train and test data\n",
        "train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
        "test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n",
        "print(len(train_data))\n",
        "print(len(test_data))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-18T13:45:50.452008Z",
          "iopub.execute_input": "2024-09-18T13:45:50.452637Z",
          "iopub.status.idle": "2024-09-18T13:45:50.488947Z",
          "shell.execute_reply.started": "2024-09-18T13:45:50.452594Z",
          "shell.execute_reply": "2024-09-18T13:45:50.48751Z"
        },
        "trusted": true,
        "id": "fveGuoSnwXL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-processing our train and test data\n",
        "# Step 1: Drop the 'Name', 'Fare', 'Ticket', and 'Cabin' fields; we feel they are not necessary for predictions and/or are unique IDs\n",
        "train_data = train_data.drop([\"Name\", \"Fare\", \"Ticket\", \"Cabin\"], axis=1)\n",
        "test_data = test_data.drop([\"Name\", \"Fare\", \"Ticket\", \"Cabin\"], axis=1)\n",
        "print(len(train_data))\n",
        "print(len(test_data))\n",
        "# train_data.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-18T13:46:06.854254Z",
          "iopub.execute_input": "2024-09-18T13:46:06.855375Z",
          "iopub.status.idle": "2024-09-18T13:46:06.87706Z",
          "shell.execute_reply.started": "2024-09-18T13:46:06.855317Z",
          "shell.execute_reply": "2024-09-18T13:46:06.875669Z"
        },
        "trusted": true,
        "id": "mcCaDNftwXL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Handle missing values; we will replace any null/NA values with mean of its column\n",
        "# To do this, let's check which columns in our dataset have any such values\n",
        "null_columns_train = train_data.isnull().any()\n",
        "null_columns_test = test_data.isnull().any()\n",
        "\n",
        "# Display columns with null values\n",
        "columns_with_nulls_train = null_columns_train[null_columns_train].index\n",
        "print(\"Columns with null values in training set:\", columns_with_nulls_train.tolist())\n",
        "\n",
        "columns_with_nulls_test = null_columns_test[null_columns_test].index\n",
        "print(\"Columns with null values in testing set:\", columns_with_nulls_test.tolist())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-18T13:46:12.827622Z",
          "iopub.execute_input": "2024-09-18T13:46:12.828062Z",
          "iopub.status.idle": "2024-09-18T13:46:12.83889Z",
          "shell.execute_reply.started": "2024-09-18T13:46:12.828018Z",
          "shell.execute_reply": "2024-09-18T13:46:12.837697Z"
        },
        "trusted": true,
        "id": "a7w3GvZnwXL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update missing values in 'Age' with the mean of the column\n",
        "train_data.fillna({'Age': train_data['Age'].mean()}, inplace=True)\n",
        "test_data.fillna({'Age': test_data['Age'].mean()}, inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-18T13:46:18.421128Z",
          "iopub.execute_input": "2024-09-18T13:46:18.422418Z",
          "iopub.status.idle": "2024-09-18T13:46:18.431591Z",
          "shell.execute_reply.started": "2024-09-18T13:46:18.42236Z",
          "shell.execute_reply": "2024-09-18T13:46:18.430501Z"
        },
        "trusted": true,
        "id": "iA_AGyhBwXL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop missing values in 'Embarked', as they may cause errors in prediction if not dropped\n",
        "train_data = train_data.dropna()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-18T13:46:39.154271Z",
          "iopub.execute_input": "2024-09-18T13:46:39.154671Z",
          "iopub.status.idle": "2024-09-18T13:46:39.163521Z",
          "shell.execute_reply.started": "2024-09-18T13:46:39.154633Z",
          "shell.execute_reply": "2024-09-18T13:46:39.162244Z"
        },
        "trusted": true,
        "id": "HZwmEgXGwXL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check to ensure no more missing values now\n",
        "null_columns_train = train_data.isnull().any()\n",
        "null_columns_test = test_data.isnull().any()\n",
        "\n",
        "# Display columns with null values\n",
        "columns_with_nulls_train = null_columns_train[null_columns_train].index\n",
        "print(\"Columns with null values in training set:\", columns_with_nulls_train.tolist())\n",
        "\n",
        "columns_with_nulls_test = null_columns_test[null_columns_test].index\n",
        "print(\"Columns with null values in testing set:\", columns_with_nulls_test.tolist())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-18T13:46:42.228113Z",
          "iopub.execute_input": "2024-09-18T13:46:42.228816Z",
          "iopub.status.idle": "2024-09-18T13:46:42.238045Z",
          "shell.execute_reply.started": "2024-09-18T13:46:42.228774Z",
          "shell.execute_reply": "2024-09-18T13:46:42.236799Z"
        },
        "trusted": true,
        "id": "hyNsVaHRwXL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Perform OHE on categorical fields\n",
        "encoded_train_data = pd.get_dummies(train_data, columns=['Sex', 'Embarked',])\n",
        "encoded_test_data = pd.get_dummies(test_data, columns=['Sex', 'Embarked',])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-18T13:47:05.707005Z",
          "iopub.execute_input": "2024-09-18T13:47:05.707434Z",
          "iopub.status.idle": "2024-09-18T13:47:05.725201Z",
          "shell.execute_reply.started": "2024-09-18T13:47:05.707395Z",
          "shell.execute_reply": "2024-09-18T13:47:05.724028Z"
        },
        "trusted": true,
        "id": "rh3zNb1uwXL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Normalize numerical data; we will be using min-max scaling\n",
        "# Copy data\n",
        "train_min_max_scaled = encoded_train_data.copy()\n",
        "test_min_max_scaled = encoded_test_data.copy()\n",
        "\n",
        "# Function for normalizing the data using min-max scaling\n",
        "def minMaxScale(df, column):\n",
        "  df[column] = (df[column] - df[column].min()) / (df[column].max() - df[column].min())\n",
        "\n",
        "# Normalizing the training data\n",
        "minMaxScale(train_min_max_scaled, 'Pclass')\n",
        "minMaxScale(train_min_max_scaled, 'Age')\n",
        "minMaxScale(train_min_max_scaled, 'SibSp')\n",
        "minMaxScale(train_min_max_scaled, 'Parch')\n",
        "\n",
        "# Normalizing the testing data\n",
        "minMaxScale(test_min_max_scaled, 'Pclass')\n",
        "minMaxScale(test_min_max_scaled, 'Age')\n",
        "minMaxScale(test_min_max_scaled, 'SibSp')\n",
        "minMaxScale(test_min_max_scaled, 'Parch')\n",
        "\n",
        "# View normalized data\n",
        "# display(train_min_max_scaled)\n",
        "# display(test_min_max_scaled)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-18T13:47:52.359898Z",
          "iopub.execute_input": "2024-09-18T13:47:52.360384Z",
          "iopub.status.idle": "2024-09-18T13:47:52.37749Z",
          "shell.execute_reply.started": "2024-09-18T13:47:52.360342Z",
          "shell.execute_reply": "2024-09-18T13:47:52.376271Z"
        },
        "trusted": true,
        "id": "yxxEGACOwXL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing logistic regression using sklearn library\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize the model and our pre-processed dataset; also rename testing data for consistency sake\n",
        "our_log_reg = LogisticRegression(max_iter=1000)\n",
        "X_train = train_min_max_scaled.drop('Survived', axis=1)\n",
        "y_train = train_min_max_scaled['Survived']\n",
        "X_test = test_min_max_scaled\n",
        "\n",
        "# Train the model on our training data\n",
        "our_log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict targets of testing data\n",
        "y_test_pred = our_log_reg.predict(X_test)\n",
        "\n",
        "# View our predictions\n",
        "passenger_ids = test_data['PassengerId']\n",
        "output = pd.DataFrame({'PassengerId': passenger_ids, 'Survived': y_test_pred})\n",
        "# output\n",
        "output.to_csv('submission.csv', index=False)\n",
        "print(\"Your submission was successfully saved!\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-18T13:54:26.828518Z",
          "iopub.execute_input": "2024-09-18T13:54:26.828994Z",
          "iopub.status.idle": "2024-09-18T13:54:26.901669Z",
          "shell.execute_reply.started": "2024-09-18T13:54:26.828915Z",
          "shell.execute_reply": "2024-09-18T13:54:26.900486Z"
        },
        "trusted": true,
        "id": "EGpl9g4mwXL_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}